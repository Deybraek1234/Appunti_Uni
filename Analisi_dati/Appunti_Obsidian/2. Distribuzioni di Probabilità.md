# Distribuzione Binomiale
La binomiale si usa nei casi dove ho due risultati possibili, di successo e fallimento. 
>[!def] Binomiale
> Definito:
> 	* p  -- probabilità di successo
> 	* q=1-p  -- probabilità di fallimento
> 	* n  -- ripetizioni dello sperimento
> 	* k  -- quanti eventi favorevoli su n si avrà
>  la probabilità di avere k eventi favorevoli su n ripetizioni è dato da 
>  $$
> P(k,n,p) = p^k (1-p)^{n-k} \frac{n!}{k!(n-k)!} = p^k(1-p)^{n-k} \binom{n}{k}
>$$

Per ricavare la funzione generatrice dei momenti algebrici, dobbiamo fare l'osservazione che la binomiale tratta casi discreti, quindi dovremmo usare la definizione discreta.
>[!thm] La funzione generatrice dei momenti algebrici della binomiale
> $$
> M_{k}(t) = \mathbb{E}[e^{tk}] = \sum_{k=0}^{n} e^{tk} p^k (1-p)^{n-k} \binom{n}{k} = \sum_{k=0}^{n} (e^tp)^kq^{n-k} = (pe^t+q)^n
>$$
>dove nell'ultimo passaggio si è usato il binomio di newton $(a+b)^n = \sum_{x=0}^{n}\binom{n}{x}a^xb^{n-x}$ 

Nello stesso modo si può ricavare la funzione generatrice dei momenti centrali
>[!thm] La funzione generatrice dei momenti centrali della binomiale
> $$
> \bar{M}_{x}(t) = \mathbb{E}[e^{t(k-\mu)}] = e^{t(x-\mu)}(pe^t + q)^k = (e^t(1-p)p + e^{-tp}q)^k = (pe^{qt} + qe^{-pt})^k
>$$

E una volta noti questi, possiamo ricavarci i momenti algebrici e momenti centrali derivando e valutando in t=0:
>[!mgf] I momenti algebrici della binomiale
> 1. $\mu_{0}$ non è altro che la funzione valutata in t=0 risulta essere $\mu_{0} = 1$
> 
> 2. Il valore di aspettazione risulta essere
> $$
> \mu_{1} = \left.\frac{\partial M_{x}}{\partial t} \right |_{t=0} = n(pe^t+q)^{n-1}pe^t = n(p+1-p)^{n-1}p = np
>$$

>[!central-mgf] I momenti centrali della binomiale
>1.  $\bar{\mu}_{0} = 1$
>
> 2. Il valore di aspettazione momento centrale (per definizione?)
> $\bar{\mu}_{1} = 0$
> 3. Il valore di aspettazione si può calcolare usando la proprietà della varianza
> $$
> \text{var(x)} = \mathbb{E}[x^2] - \mathbb{E}[x]^2
> $$
> Derivando per la prima volta:
> $$
> \begin{gather}
> \left.\frac{\partial M_{x}}{\partial t} \right |_{t=0} = -npe^{-npt}(q+pe^t)^n + e^{-npt}n(q+pe^t)^{n-1}pe^t = \\  \\
> = ne^{-npt}(q+pe^t)^{n-1}[-p(q+pe^t) + pe^t]
> \end{gather}
>$$
>Derivando la seconda volta, otteniamo che 
> $$
> \text{var(x)} = \bar{\mu}_{2} = \left.\frac{\partial^2 M_{x}}{\partial t^2} \right |_{t=0} = np(1-p) = npq
> $$
>
> 4.  $\bar{\mu}_{3} = npq(1-2p)$
> In questo modo, se p=0.5 abbiamo che il momento terzo = 0 e allora la distribuzione è simmetrica. Mentre se p = 1, avremmo che il momento terzo è minore di zero e avrà una coda verso i x decrescenti. Se p = 0, accadrà la stessa cosa ma per x crescenti. 
> 5. $\bar{\mu}_{4} = npq(1-3pq(2-n))$
i


26/02/2026
# Distribuzione di Poisson
Voglio sapere quale è la probabilità di ottenere k eventi però la applichiamo nel caso che abbiamo un particolare risultato che mi interessa e voglio misurare quante volte si verifica. È per sapere quanti eventi n si verificano sapendo che in media si verifica p volte. 

!!! Riguarda
Mantenendo dalla binomiale np=cost, ovvero per n->$\infty$ e p->0 si ottiene la distribuzione di Poisson. Si chiama anche distribuzione degli eventi rari. 
Prendiamo 
$$
\lim_{ n \to \infty, p \rightarrow 0} \text{Binomiale}
$$
>[!thm] La distribuzione di Poisson si ottiene facendo il limite della Binomiale
>Ho un intervallo di tempo $\Delta T$ molto largo con $\lambda$ eventi. Suddivido il mio intervallo $\Delta T$ in tanti intervallini più piccoli $\Delta t$. So che la probabilità di un evento di cadere in un intervallino è $p=\frac{\lambda}{N}$ dove N è il numero di intervalli totali (assunti indipendenti uno dal altro). 
>Partendo dalla binomiale
>$$
>P\left( k,n, \frac{\lambda}{n} \right) = \frac{\lambda^k}{n^k} \left( 1- \frac{\lambda}{n} \right)^{n-k} \frac{n!}{k!(n-k)!}
>$$
>posso passare al limite per n $\rightarrow \infty$ (ipotesi per Poisson)
>$$
> \frac{n!}{(n-k)!} = \frac{n(n-1)\dots(n+k+1)(\cancel{ n-k)!}}{\cancel{ (n-k)! }} \xrightarrow{n \rightarrow \infty} n^k
>$$
> Inoltre possiamo riscrivere il termine riconoscendo la serie di taylor
>$$
> \left( 1- \frac{\lambda}{n} \right)^{n-k} \rightarrow \left( 1 - \frac{\lambda}{n)} \right)^n \xrightarrow{n \rightarrow \infty} e^{ -\lambda }
>$$
> E quindi possiamo scrivere la probabilità P come 
>$$
> P\left( x,n, \frac{\lambda}{n} \right) = \frac{\lambda^k}{\cancel{n^k}}e^{ -\lambda } \cancel{n^k} \frac{1}{k!} = \frac{e^{ -\lambda }\lambda^k}{k!}
>$$
>dove $0\leq k \leq\infty$. 

Un processo che è una binomiale ma che tratto come una distribuzione di Poisson è il decadimento radioattivo. 
!!!

>[!def] La distribuzione di Poisson
>Nel caso continuo si ha che la distribuzione di Poisson 
> $$
> P(k, \lambda) = \frac{\lambda^k}{k!}e^{-\lambda} 
> $$

>[!thm] Funzione generatrice di momenti algebrici
>La funzione generatrice di momenti algebrici è 
>$$
>\begin{gather}
> M_k (t) = \mathbb{E}[e^{ tk }] = \sum_k e^{ tk } \frac{\lambda^k}{k!}e^{ -\lambda } = e^{ -\lambda }\sum_k \frac{(e^{ t }\lambda)^k}{k!} = e^{ \lambda (e^t -1)}
> \end{gather}
>$$
>dove nell'ultimo passaggio si ha usato il sviluppo in serie di McLaurin dell'esponenziale dato da 
>$$
> e^{ x } = \sum_{n=0}^{\infty} \frac{x^n}{n!} 
>$$

>[!thm] La funzione generatrice di momenti centrali 
>La funzione generatrice di momenti centrali è
>$$
> \bar{M}_k (t) = \lambda e^{ t }e^{ \lambda(e^{ t } -1) }
>$$

Quindi una volta stabilite queste funzioni, avremmo che i 
>[!mgf] I momenti algebrici
> 1. $\mu_{0}$ = 1
> 2. $\mu_{1}$ = $\lambda$
> 3. $\mu_{2} = \lambda$

>[!central-mgf] I momenti centrali
>4. $\bar{\mu}_{0} = 1$
>5. $\bar{\mu}_{1} = 0$
>6. $\bar{\mu}_{2} = \nu$
>7. $\bar{\mu}_{3} = \nu$
>8. $\bar{\mu}_{4} = 3\nu^{2}+\nu$

## Istogrammi
Un'applicazione di questa distribuzione è nella costruzione degli istogrammi. Se consideriamo il k-esimo intervallo di ampiezza dx centrato nel valore $x_{k}$, se prendiamo N misure, otteniamo un numero di valori id ciascun bin $n_{k}^m$.
Questo valore avrà una distribuzione binomiale (visto che o può capitare dentro o fuori l'intervallo), quindi avremmo come il valore di aspettazione del k-esimo bin:
$$
\mathbb{E}[n^m_{k}] = Np_{k}
$$
e come varianza 
$$
\text{Var}(n^m_k) = Np_{k}(1-p_{k})
$$ 
dove $p_{k}$ è la probabilità che il singolo evento sia favorevole, ovvero la probabilità che cada nell'intervallo considerato. Avremo che 
$$
p_{k} = \int _{x_{k}}^{x_{k}+ \Delta x} P(x)\, dx \approx P(x_{k})\Delta x
$$
Se abbiamo un numero elevato di eventi (e quindi anche numero di intervalli) la probabilità che il singolo evento cada nell'intervallo tende a zero e quindi si può assumere che il numero di eventi in un intervallo segue la distribuzione di Poisson. In questo caso avremmo 
$$
\begin{gather}
\mathbb{E}[n^m_{k}] = \nu = Np_{k}  \\
\text{Var}(n^m_{k}) = \nu = Np_{k}
\end{gather}
$$
se non si conosce la funzione di distribuzione della variabile casuale, possiamo usare la definizione frequenzista di probabilità, ovvero dove $P_{k} = \frac{n^m_{k}}{N}$, ovvero dove abbiamo eventi favorevoli fratto eventi totali. In questo caso risulterà che il valore di aspettazione e la varianza sono 
$$
\begin{gather}
\mathbb{E}[n^m_{k}] = n^m_{k} \\
\text{Var}(n^m_{k}) = n^m_{k}
\end{gather}
$$

# Distribuzione di Gauss
La distribuzione di Gauss viene anche detto la distribuzione normale, e viene descritta dalla funzione 
>[!def] La distribuzione di Gauss
>La distribuzione di Gauss ha la forma
>$$
>f(x, \sigma, \mu) = \frac{1}{\sqrt{ 2\pi} \sigma}e^{ -\frac{(x-\mu)^{2}}{2\sigma^{2}} }
>$$

>[!thm] La funzione generatrice di momenti algebirici
>La funzione generatrice di momenti algebrici della gaussiana è uguale a 
>$$
>\begin{gather}
> M_x (t) = \mathbb{E}[e^{ t(x-\mu) }] = \frac{1}{\sqrt{2\pi }\sigma} \int_{-\infty}^{\infty} e^{ t(x-\mu) - \frac{(x-\mu)^2}{2\sigma^{2}} } \, dx  \\ 
> = \frac{1}{\sqrt{ 2\pi }\sigma} \int_{-\infty}^{\infty} e^{ \frac{-y^{2}}{2\sigma^{2}} } e^ \frac{{\sigma^{2}}t^2}{2} \, dx  \\
> = e^{ \frac{\sigma^{2}t^{2}}{2} }
> \end{gather}
>$$
>dove abbiamo posto 
>$$
>\begin{gather}
> \frac{(x-\mu)^2}{2\sigma^{2}} - t(x-\mu) = \frac{1}{2\sigma^{2}}[(x-\mu)^{2} + 2\sigma^{2}t(x-\mu) + \sigma^4t^2-\sigma^4t^{2}] \\
> = \frac{1}{2\sigma^{2}} \left[ y^{2} - \frac{\sigma^{4} t^2}{2} \right] \\
> = \frac{y^{2}}{2\sigma^4} - \frac{\sigma^{2}t^{2}}{2}
>\end{gather}
>$$

>[!thm] La funzione generatrice di momenti centrali
>La funzione generatrice di momenti centrali della gaussiana possiamo usare la proprietà 
>$$
> \bar{M}_x (t) = M_x (t) e^{\mu t} = e^{( \mu t + \sigma^{2}t^{2}/2 )}
>$$

Quindi i momenti algebrici ci risultano essere 
>[!mgf] I momenti algebrici sono
>1. $\mu_{0} = 1$
>2. $\mu_{1} = \mu$

>[!central-mgf] I momenti centrali sono
>1. $\bar{\mu}_{0} = 1$
>2. $\bar{\mu}_{1} = 0$
>3. $\bar{\mu}_{2} = \sigma^{2}$
>4. $\bar{\mu}_{3} = 0$
>5. $\bar{\mu}_{4} = 3\sigma^4$

# Funzione Gamma
Questa funzione dipende da due parametri $\alpha$ e $\beta$, dove $x \in \mathbb{R} > 0$. 
>[!def] La funzione gamma è definita come 
>$$
> f(x, \alpha, \beta) = \frac{1}{\beta^\alpha  \Gamma(\alpha)} x^{\alpha-1}e^{ \frac{-x}{\beta} }
>$$
>dove con la $\Gamma(\alpha)$ indichiamo
>$$
>\begin{gather}
> \Gamma(\alpha) = \int_{0}^\infty z^{\alpha-1} e^{ -z } \; dz \\ 
> \Gamma(1) = \int_{0}^\infty e^{ -z } \; = 1  \\
> \Gamma\left( \frac{1}{2} \right) = \int_{0}^\infty e^{ -z }z^{-\frac{1}{2}} \; dz = \sqrt{ \pi } \; \; \; \; \text{avendo posto} z=\frac{y^{2}}{2} \\
> \Gamma(\alpha+1) = \int_{0}^\infty z^{\alpha-1} z e^{ -z } \; dz = \int_{0}^\infty z^{\alpha} e^{ -z } \; dz  \\
> = [z-^{\alpha} e^{ -z }]_{0}^\infty + \alpha \int_{0}*\infty z^{\alpha-1} e^{ -z } \; dz  \\
> = \alpha \Gamma(\alpha)
> \end{gather}
>$$ 
>dove nel caso di un numero intero abbiamo che $\Gamma(n+1) = n!$

La distribuzione gamma è:
* normalizzata: $\int_{0}^\infty x^{\alpha-1} e^{ -\frac{x}{\beta} } \; dx = \beta^\alpha \Gamma(\alpha)$
* ha valore di aspettazione $\mathbb{E}[x]=\alpha\beta$
* ha varianza $\text{Var}(x) = \beta^{2}\alpha$

# Distribuzione di Erlang
È un caso particolare della funzione di gamma con $\alpha = k$, dove $k \in \mathbb{Z}$, e $\beta = 1$. 
>[!def] La distribuzione di Erlang
>La distribuzione di Erlang segue la funzione
>$$
> f(x, k) = \frac{1}{(k-1)!} x^{k-1} e^{ -x }
>$$

Ha valore di aspettazione 
$$
\mathbb{E}[x] = \frac{1}{(k-1)!} \underbrace{ \int_{0}^\infty x^k e^{ -x } \; dx }_{ \Gamma(k+1)=k! } = k
$$
e ha varianza 
$$
\text{Var}(x) = \mathbb{E}[x^2] - \mathbb{E}[x]^2 = k
$$
il che segue dal fatto che 
$$
\mathbb{E}[x^2] =\frac{1}{(k-1)!}\underbrace{ \int_{0}^\infty x^{k+1} e^{ -x } \; dx }_{ \Gamma(k+2) = (k+1)! } = k^2 + k
$$

# Distribuzione dei tempi d'attesa
