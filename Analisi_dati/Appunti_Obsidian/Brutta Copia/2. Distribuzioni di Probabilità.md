# Binomiale (di Bernoulli)
Questa distribuzione si usa nei casi dove ho due risultati possibili (come lanciare una moneta). Ho o successo o fallimento (favorevole/sfavorevole, etc). In generale di questi due casi, indico con p la probabilità del successo e q=1-p come la probabilità del caso sfavorevole, con k<n. Ripetuto n volte lo sperimento con probabilità k successi:
$$
\begin{gather}
P(k, n, p) = p^k (1-p)^{n-k} \frac{n!}{k!(n-k)!} = p^k (1-p)^{n-k} \binom{n}{k}
\end{gather}
$$
Se ho tre oggetti ABC, e di questi voglio stabilire in quanti modi diversi ne posso prendere 2, n=3, k=2 avrò che posso prendere: AB, BC, AC e quindi 3 modi diversi che posso prendere questi oggetti. Fatto il conto $\frac{3!}{2!(3-2)!} = 3$. 
Questa è una distribuzione di variabile discreta e quindi calcolo la mia funzione generatrice di momenti usando la definizione nel caso discreto:
$$
M_{k}(t) = \sum_{k=0}^n e^{tk} p^k (1-p)^{n-k} \binom{n}{k}
$$
Facendo riferimento al binomio di Newton $(a+b)^n = \sum_{x=0}^n \binom{n}{x}a^xb^{n-x}$. E quindi lo posso riscrivere come:
$$
M_{k}(t) = \sum_{k=0}^n (pe^t)^k = (pe^t + q)^n
$$
Prendendo la derivata prima e valutandola in t=0, avrò che 
$$
\frac{ \partial M_{x} }{ \partial t } (t=0) = n(pe^t + q)^{n-1} pe^t = n(p+1-p)^{n-1} p = np \rightarrow \mu
$$
il che non è altro che il valore di aspettazione della distribuzione. 
Calcolando la varianza usando la definizione:  $$
\text{var(x)} = E[x^2] - E^2[x]
$$
Prima ci dobbiamo trovare $E[x^2]$:
$$
\begin{gather}
M_{k}^{'}(t) = npe^t (pe^t+q)^{n-1} = npe^t(pe^t +q)^{n-1}+npe^t(n-1)(pe^t+q)^{n-2}pe^t \\
M_{k}^{'}(t=0) = np(p+1-p)^{n-1} + np(n-1)(p+1-p)^{n-2}p = np+np^2 = E[x^2]
\end{gather}
$$
e quindi il conto risulta $$
\text{var(x)} = np+np^2(n-1)-(np)^2 = np+n^2p^2 -np^2-n^2p^2 = np(1-p)=npq = var(k)
$$
???????

Per la binomiale:
* $\bar{\mu_{3}} = npq(1-2p)$
	* se p = 0.5, allora vuoldire che il momento terzo = 0 e allora è simmetrico
	* se invece p = 1, allora il momento terzo è minore di zero e allora avrà una coda per x negativi. 




# 26/02/2026
Funzione generatrice di momenti della distirbuzione di Poisson

  Partendo dalla funzione generatrice di momenti della binomiale 
  $$
M_k (t) = (pe^{ t }+q)^n = (pe^{ t }1-p)^n = [1+p(e^{ t }-1)]^n
$$
e sapendo che per definizione $p = \frac{\lambda}{n}$  possiamo riscriverlo come 
$$
[ 1+\frac{\lambda}{n}(e^{ t }-1)]^n
$$
e prendendo il limite per $n \rightarrow \infty$ 
$$
	\lim_{ n \to \infty } M_x^B (t) = e^{ \lambda(e^{ t }-1) } = M_k^{\text{Poiss}} (t) 
$$

La varianza della binomiale $\text{Var}^B(x) = npq= np(1-p) = np-np^{2} < np = \lambda = {\text{Var}^\text{Poiss}}(x)$ è minore di quella della Poissoniana. 

Supponiamo di avere due variabili aleatori con densità di probabilità poissoniana $\lambda_{a}$ $\lambda_{d}$ e per qualche motivo non riesco a distinguere il numero di occorrenze delle due singoli processi e posso solo calcolare $k=k_{a}+k_{b}$. Quindi non posso specificare singolarmente cosa succede, ma solo la somma di tutti. Se voglio calcolarmi $P(k_{a}+K_{b}, \lambda_{a}, \lambda_{b})$ e per trovare questo posso usare il teoremino di prima dove 

>[!thm] Se ho z=x+y, la $M_z (t) = M_x M_y$. 

Quindi prendendo 
$$
\begin{gather}
M_a (t) = e^{ \lambda_{b} (e^{ t }-1) } \\
M_b (t) = e^{ \lambda_{b}(e^{ t }-1) } \\
M_{a+b} (t) = M_a M_b = e^{ (e^{ t }-1) (\lambda_{a} + \lambda_{b})}
\end{gather}
$$
>[!rmk]  Osservazione: Dato z = x+y, posso dire che $\lambda_{z} = \lambda_{a}+\lambda_{b}$. 


## Istogrammi
Dato un certo insieme di eventi $N = \{x_{1}, x_{2}, \dots, x_{n}\}$ avrò che la probabilità di ciascun 