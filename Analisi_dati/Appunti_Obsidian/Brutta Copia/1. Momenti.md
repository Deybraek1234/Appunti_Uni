Il valore di aspettazione è definito come 
$$
\begin{gather}
E[x] = <x> = \int_{\Omega x} xP(x) dx = \mu \\ 
E[(x- \mu)^2] = <(x-\mu)^2> = \int _{\Omega x} (x-\mu)^2 P(x) dx = \text{var(x)}
\end{gather}
$$
questi sono dei casi specifici dei numeri che si chiamano momenti. Il momento di ordine K è definito come $$
\mu_{k} = E[x^k] = \int_{\Omega x} x^k P(x) dx
$$
con il caso specifico di: 
* k=0 $\rightarrow$ $\mu_{0} = \int _\Omega x$ P(x)dx = 1
* k=1 $\rightarrow$ $\mu_{1} = \mu$

## Momenti centrali
Oltre ai momenti così definiti esistono i momenti centrali:$$
\bar{\mu_{k}} = E[(x-\mu)^k]= \int_{\Omega x} (x-\mu)^k P(x) dx
$$
con casi specifici:
* k=0 $\rightarrow$ $\bar{\mu_{0}}$
* k=1 $\rightarrow$ $\bar{\mu_{1}} = \int_{ \Omega x} (x-\mu) P(x) dx = \mu - \mu = 0$ (passaggi vengono dal distribuire e dividere l'integrale)
* k=2 $\rightarrow$ $\int_{\Omega x} (x-\mu)^2 P(x) dx = \text{var(x)}$
* k=3 $\rightarrow$ $\bar{\mu_{3}} = \int _{\Omega x} (x-\mu)^3 P(x) dx$ (indice di asimmetria)
	* questo valore risulta essere circa 0 per distribuzioni simmetriche
	* per $\bar{\mu_{3}} \geq 0$ allora si ha distribuzione con coda per x $\rightarrow$ inf
* k=4 $\rightarrow$ $\bar{\mu_{4}} = \int_{\Omega x} (x-\mu)^4 P(x) dx$ (indice di Kurtosi)
	* ci da quanto peso hanno gli outlier sulla distribuzione

Dato due variabili aleatori: x,y: se questi hanno gli stessi momenti, allora le distribuzioni di probabilità $P_{x} =P_{y}$. Un altro modo per dimostrare che due distribuzioni sono uguali è di usare la funzione generatrice di momenti. La funzione di per se si ricava con un integrale, ma si può calcolare relativamente semplicemente, e una volta nota tutti i momenti si possono calcolare con le derivate. 

Definiamo la funzione generatrice di momenti come:
$$
\begin{gather}
M_{x}(t) = E[e^{tx}] = \int_{\Omega x} e^{tx} P(x) dx \; \; \; \text{(caso continuo)}\\
M_{x}(t) = \sum_{i} p_{i} e^{tx_{i}} \; \; \; \text{(caso discreto)}
\end{gather}
$$
dove t è una variabile ausilaria. 

$$
[ \frac{{\partial}^kM_{x}}{\partial t^k}]_{t=0} = E[x^n] = \mu_{n}
$$
Per dimostrarlo uso la serie di McLaurin:
$$
e^{tk} = \sum_{k=0}^{+inf} \frac{(tx)^k}{k!} \rightarrow M_{x}(t) = \int_{\Omega x} \sum_{i}^{+inf} \frac{(tx)^k}{k!} P(x)dx
$$
commuto somme e integrali e scopro che l'integrale è $\mu_{x}$
$$
= \sum_{k=0}^{+inf} \frac{t^k}{k!} \mu_{k} = \mu_{0} + \mu_{1}t + \mu_{2} \frac{t^2}{2} + \dots
$$
Ora se derivo questa espressione rispetto a t, avrò che 
$$
\frac{{\partial M_{x}}}{\partial t} = \mu_{1} + t\mu_{2} + \dots
$$
il che valutato in t=0 ci da il momento k=1 in questo caso e k in altri casi generali.

Ci sono anche le funzioni generatrici di momenti centrali che è definito come:
$$
\begin{gather}
\bar{M_{x}(t)} = E[e^{t(x-\mu)}] = \int_{\Omega x} e^{t(x-\mu)} P(x) \; \; \; \text{(caso continuo)} \\ \\
\bar{M_{x})(t)} = \sum_{i}p_{i}e^{t(x_{i} - \mu)}
\end{gather}
$$
e preso la derivata n-esima rispetto a t mi da $\bar{\mu_{n}}$. Inoltre posso portare fuori dall'integrale:
$$
e^{-\mu t} \int_{\Omega x} e^{tx} P(x)dx = e^{-\mu t} M_{x}(t)
$$
E questo si può usare per passare dalle funzioni generatrici a... 

Se posso dimostrare che le variabili alleatori x, y hanno la stessa funzione $M_{x}(t)$ o $\bar{M_{i}(t)}$ allora avranno la stessa distribuzione di probabilità.

Inoltre esiste un'altra funzione chiamata funzione caratteristica:
$$
\phi(x) = E[e^{itx}] = \int_{\Omega x} e^{itx} P(x) dx
$$
e questa funzione caratteristica è unica. 

Una proprietà interessante è che se ho una variabile alleatoria, data dalla somma di due variabili alleatorie indipendenti z=x+y, allora $M_{z}(t) = M_{x}(t)M_{y}(t)$. Si può dimostrare prendendo $P_{z}$ densità di probabilità, allora avrò che $P_{z} = P_x P_y$. E quindi avrò che $$
M_{z}(t) = \int _{\Omega x} e^{tx} P(x)dx \; \int_{\Omega y} e^{ty} P(y) dy = M_{x}(t) M_{y} (t)
$$
