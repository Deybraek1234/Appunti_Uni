Definiamo il valore di aspettazione come nel caso continuo come:$$
\mathbb{E}[x] = <x> \int_{\Omega_{x}}\;xP(x) \; dx =\mu
$$
E la varianza come:
$$
\mathbb{E}[(x-\mu)^2] = <(x-\mu)^2> = \int_{\Omega_{x}} \; (x-\mu)^2P(x) \; dx
$$
Ma questi sono casi specifici di numeri che chiamiamo momenti. 
# Momenti
>[!def] Momento algebrico di ordine k
Il momento di ordine K è definito come:
> $$
\mu_{k} = \mathbb{E}[x^k] = \int_{\Omega_{x}} \; x^k P(x) \; dx 
> $$

Possiamo inoltre definire i 
## Momenti Centrali
>[!def] Momento Centrale di ordine k
I momenti centrali sono definiti come:
> $$
\bar{\mu}_k = \mathbb{E}[(x-\mu)^k] = \int_{\Omega_{x}} \; (x-\mu)^k P(x) \; dx  
> $$

con i casi specifici:
* k=0 $\rightarrow$ $\bar{\mu}_{0}$ 
* k=1 $\rightarrow$ $\bar{\mu}_{1}$ (valore atteso)
* k=2 $\rightarrow$ $\bar{\mu}_{2}$ (varianza)
* k=3 $\rightarrow$ $\bar{\mu}_{3}$ (indice di assimetria)
	* per distribuzioni simmetriche viene 0. Per $\bar{\mu}_{3}\geq 0$ si ha distribuzioni con coda verso destra, e verso sinistra per $\bar{\mu}_{3}\leq 0$. 
* k=4 $\rightarrow$ $\bar{\mu}_{4}$ (indice di Kurtosi)
	* ci da quanto peso hanno gli outlier sulle distribuzioni. 

Un risultato interessante è che il momento centrale del valore medio (k=2) è sempre zero. Infatti: 
>[!thm] Il momento centrale per k=1 è nullo
>Usando la definizione del valore di aspettazione:
>$$
> \bar{\mu}_{1} = \int_{\Omega_{x}} \;(x-\mu)P(x) \; dx = \int_{\Omega_{x}} \; x P(x)  \; dx - \int_{\Omega_{x}} \; \mu P(x) \; dx = \mu - \mu \int_{\Omega_{x}} \; P(x) \; dx  = 0   
> $$
> Che segue dal fatto che 
> $$ \int_{\Omega_{x}} \; P(x) \; dx = 1
> $$

>[!thm] Dato due variabili aleatori, se hanno gli stessi momenti allora le distribuzioni di probabilità sono uguali
>Se prendiamo due variabili aleatori: x e y, con rispettive distribuzioni di probabilità $P_{x}$ e $P_y$, se le variabili hanno gli stessi momenti, allora li distribuzioni di probabilità sono uguali. 

Questo ultimo teorema si può anche dimostrare in modo più facile definendo la funzione generatrice di momenti. 
>[!def] Funzione generatrice di momenti
> Definiamo la funzione generatrice di momenti nel caso continuo come come:
> $$
M_{x}(t) = \mathbb{E}[e^{tx}] = \sum_{i} p_{i}e^{tx_{i}}
> $$
> mentre nel caso continuo lo definiamo come:
> $$
> M_{x}(t) = \mathbb{E}[e^{tx}] = \int_{\Omega_{x}} \; e^{tx}P(x) \; dx 
> $$
> dove t è una variabile ausilaria

Da questa definizione segue un importante risultato, ovvero che possiamo derivare questa funzione k volte per ottenere il momento di ordine k. 
>[!thm] La derivata k-esima della funzione generatrice di momenti ci da il momento k
>Si può esprimere questo risultato come:
>$$
> \left.\frac{ \partial^k M_{x} }{ \partial t^k } \right |_{t=0} = \mathbb{E}[x^k] = \mu_{k}
>$$
>Per dimostrarlo prendiamo la serie di McLaurin dell'esponensiale:
> $$
> e^{tk} = \sum_{i}^{\infty} \frac{(tx)^k}{k!}
> $$
> che messo nella definizione di funzione generatrice di momenti risulta essere:
> $$
> \begin{gather}
> M_{k}(t) = \int_{\Omega_{x}} \; \sum_{i}^{\infty} \frac{(tx)^k}{k!}P(x) \; dx = \sum_{i}^{\infty} \frac{t^k}{k!} \int_{\Omega_{x}} \; x^k P(x) \; dx  =\\
=  \sum_{i}^{\infty} \frac{t^k}{k!}\mu_{k} =  \\
= \mu_{0} + t\mu_{1} + \mu_{2} \frac{t^2}{2} + \dots
> \end{gather}
> $$
> Se deriviamo questa espressione rispetto a t, avrò che:
> $$
> \frac{ \partial M_x }{ \partial t }  = \mu_{1} + t \mu_{2} + \dots
> $$
> Il che valutato in t=0 risulta essere
> $$
> \left.\frac{\partial M_{x}}{\partial t} \right |_{t=0} = \mu_{1} + (0)\mu_{2} + \dots = \mu_{1} 
>$$ 

Non esiste solo la funzione generatrice di momenti, ma anche quella di momenti centrali
>[!def]  Funzione generatrice di momenti centrali
>La funzione generatrice di momenti centrali nel caso continuo è definito come:
> $$
> \bar{M}_{x}(t) = \mathbb{E}[e^{t(x-\mu)}] = \int_{\Omega_{x}} \; e^{t(x-\mu)} P(x) \; dx 
> $$
> ed è anche definito nel caso discreto come:
> $$
> \bar{M}_{x}(t) = \sum_{i} p_{i} e^{t(x_{i} - \mu)}
>$$

Per come è definito, abbiamo un risultato molto importante:

>[!thm]  La funzione generatrice di momenti centrali è legata alla funzione generatrice di momenti $\mu_{k}$
>Questo si può dimostrare prendendo 
>$$
> \bar{M}_{x}(t) = \mathbb{E}[e^{t(x-\mu)}] = \int_{\Omega_{x}} \; e^{t(x-\mu)}P(x) \; dx = e^{-\mu t} \int_{\Omega_{x}} \; e^{tk} P(x) \; dx = e^{-\mu t}M_{x}(t)  
> $$


E come per la funzione generatrice di momenti abbiamo che derivandola possiamo trovare i momenti della distribuzione:
>[!thm]  La k-esima derivata della funzione generatrice di momenti centrali ci da il momento k-esimo
> Partendo dalla definizione di funzione generatrice di momento 
> $$
> \bar{M}_{x}(t) = \int_{\Omega_{x}} \; e^{t(x-\mu)}P_{x} \; dx 
> $$
> e derivandola rispetto alla variabile t
> $$
> \left.\frac{\partial^k M_{x}}{\partial t^k} \right |_{t=0} = \mu_{k} 
> $$

Un risultato fondamentale è che se ho due variabili aleatori x, y, e hanno la stessa funzione generatrice di momenti o funzione generatrice di momenti centrali, allora avranno la stessa distribuzione di probabilità. 

Inoltre esiste un'altra funzione chiamata funzione caratteristica 
>[!def] Funzione Caratteristica
> $$
> \phi(x) = \mathbb{E}[e^{itx}] = \int_{\Omega_{x}} \; e^{itx}P(x) \; dx 
> $$
> e questa funzione caratteristica è unica. 

Una proprietà interessante è che se ho una variabile aleatoria data dalla somma di due variabili aleatorie indipendenti allora la funzione generatrice di momento sarà il prodotto delle due delle variabili indipendenti:
>[!thm]  Dato z=x+y, allora $M_{z} = M_{x}M_{y}$
>Dato due variabili aleatorie x, y, e z=x+y allora avrò che detto $M_{z}$ la funzione generatrice di momento, $M_{z} = M_x M_y$. Questo risultato si dimostra prendendo la densità di probabilità $P_{z}$ e notando che $P_{z} = P_{x} P_{y}$
> $$
> M_{z}(t) = \int_{\Omega_{z}} \; e^{tz}P(z) \; dz = \int_{\Omega_{x}} \; e^{tx}P(x) \; dx \int_{\Omega_{y}} \; e^{ty}P(y \; dy = M_{x}(t)M_{y}(t)   
>$$

Per la varianza e valore aspettato si deve derivare la funzione generatrice di momenti algebrici, non quella centrale

>[!rmk]  Per indicare una variabile aleatoria si usa il simbolo ~

